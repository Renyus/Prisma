# backend/app/services/chat_service.py

from __future__ import annotations

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from fastapi import HTTPException, BackgroundTasks
from sqlalchemy.orm import Session

from app.core.config import settings
from app.core.llm import call_llm, call_summary_llm
from app.core.vector_store import vector_store
from app.db import models as db_models
from app.schemas.chat import ChatRequest, ChatResponse
from app.services.payload_builder import to_openai_payload
from app.services.prompt_builder import build_normalized_prompt, _estimate_tokens
from app.crud import chat as chat_crud
from app.crud import memory as memory_crud
from app.crud import lorebook as lorebook_crud
from app.db.session import SessionLocal

# å¼ºåˆ¶æ˜¾ç¤º INFO æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é»˜è®¤é…ç½®
DEFAULT_MAX_HISTORY_MSG = 30
HARD_MAX_HISTORY_MSG = 100
DEFAULT_MAX_HISTORY_TOKENS = 2400
DEFAULT_MODEL = settings.CHAT_MODEL
# [ä¿®æ”¹] æ—¢ç„¶ä½ æœ‰ 160k çª—å£ï¼Œç›´æ¥ç»™è®°å¿†åˆ†é… 15000 tokens
# è¿™æ ·å“ªæ€•æ£€ç´¢ 50-100 æ¡çŸ­è®°å¿†ä¹Ÿèƒ½å…¨éƒ¨å¡è¿›å»
MAX_MEMORY_TOKENS = 15000

async def _run_compact_history_task(session_id: str):
    """
    åå°å‹ç¼©å†å²è®°å½•ä»»åŠ¡
    åœ¨ç‹¬ç«‹æ•°æ®åº“ä¼šè¯ä¸­æ‰§è¡Œï¼Œé¿å…ä¸»è¯·æ±‚ç»“æŸå Session è¢«å…³é—­çš„é—®é¢˜
    """
    with SessionLocal() as db:
        try:
            await _maybe_compact_history(db, session_id)
        except Exception as e:
            logger.error(f"åå°æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")
        finally:
            db.close()

async def _maybe_compact_history(db: Session, session_id: str) -> None:
    """
    åŸºäº Token æ•°é‡çš„å†å²è®°å½•å‹ç¼©
    å½“å†å²è®°å½•æ€» Token è¶…è¿‡æ¨¡å‹çª—å£çš„ 75% æ—¶è§¦å‘å‹ç¼©
    """
    # è·å–å½“å‰æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
    try:
        if hasattr(settings, "get_model_limit"):
            model_limits = settings.get_model_limit(settings.CHAT_MODEL)
            context_window = model_limits["context_window"]
        else:
            # Fallback: ä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡çª—å£
            context_window = getattr(settings, "MAX_MODEL_CONTEXT_LENGTH", 4096)
    except Exception as e:
        logger.warning(f"è·å–æ¨¡å‹é™åˆ¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡çª—å£: {e}")
        context_window = 4096

    # è®¡ç®— Token å‹ç¼©é˜ˆå€¼ï¼ˆæ¨¡å‹çª—å£çš„ 75%ï¼‰
    token_threshold = int(context_window * 0.75)
    
    # è·å–æ‰€æœ‰æœªå½’æ¡£çš„å†å²æ¶ˆæ¯ï¼ˆæŒ‰æ—¶é—´æ­£åºï¼‰
    all_messages = (
        db.query(db_models.ChatMessage)
        .filter(db_models.ChatMessage.user_id == session_id)
        .filter(db_models.ChatMessage.is_archived == False)  # åªå¤„ç†æœªå½’æ¡£çš„æ¶ˆæ¯
        .order_by(db_models.ChatMessage.created_at.asc())
        .all()
    )
    
    if not all_messages:
        return

    # è®¡ç®—æ€» Token æ•°ï¼ˆæ’é™¤æ‘˜è¦æ¶ˆæ¯ï¼‰
    total_tokens = 0
    non_summary_messages = []
    
    for msg in all_messages:
        if msg.content and not (msg.role == "system" and "æ‘˜è¦" in msg.content):
            # è®¡ç®—æ¶ˆæ¯çš„ Token æ•°ï¼ˆåŒ…å« ChatML æ ¼å¼å¼€é”€ï¼‰
            msg_tokens = _estimate_tokens(msg.content) + 4
            total_tokens += msg_tokens
            non_summary_messages.append((msg, msg_tokens))

    # å¦‚æœæ€» Token æœªè¶…è¿‡é˜ˆå€¼ï¼Œä¸éœ€è¦å‹ç¼©
    if total_tokens <= token_threshold:
        return

    logger.info(f"[{session_id}] å†å²è®°å½• Token è¶…é™: {total_tokens}/{token_threshold}ï¼Œè§¦å‘å‹ç¼©")

    # è®¡ç®—éœ€è¦å‹ç¼©çš„ Token æ•°ï¼ˆä¿ç•™ 50% çš„ç©ºé—´ï¼‰
    target_tokens = int(context_window * 0.5)
    tokens_to_compress = total_tokens - target_tokens
    
    if tokens_to_compress <= 0:
        return

    # ä»æœ€æ—§çš„æ¶ˆæ¯å¼€å§‹ï¼Œé€‰æ‹©è¦å‹ç¼©çš„æ¶ˆæ¯
    messages_to_compress = []
    compressed_tokens = 0
    
    for msg, msg_tokens in non_summary_messages:
        if compressed_tokens + msg_tokens <= tokens_to_compress:
            messages_to_compress.append(msg)
            compressed_tokens += msg_tokens
        else:
            # å¦‚æœåŠ ä¸Šè¿™æ¡æ¶ˆæ¯ä¼šè¶…å‡ºï¼Œå°±åœæ­¢
            break

    if not messages_to_compress:
        logger.warning(f"[{session_id}] æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å‹ç¼©æ¶ˆæ¯")
        return

    # å‡†å¤‡æ‘˜è¦æºæ•°æ®
    summary_sources = [
        {"role": msg.role, "content": msg.content}
        for msg in messages_to_compress
        if msg.content
    ]
    
    if not summary_sources:
        return

    try:
        # ç”Ÿæˆæ‘˜è¦
        summary_text = (await call_summary_llm(summary_sources)).strip()
    except Exception as exc:
        logger.warning(f"[{session_id}] Summary å‹ç¼©å¤±è´¥: {exc}")
        return

    if not summary_text:
        return

    # å½’æ¡£è¦å‹ç¼©çš„æ¶ˆæ¯ï¼ˆè€Œä¸æ˜¯åˆ é™¤ï¼‰
    ids_to_archive = [msg.id for msg in messages_to_compress if msg.id]
    if not ids_to_archive:
        return

    chat_crud.archive_chat_messages_by_ids(db, ids_to_archive)
    logger.info(f"[{session_id}] å·²å½’æ¡£ {len(ids_to_archive)} æ¡æ¶ˆæ¯")

    # è®¡ç®—æ‘˜è¦æ’å…¥çš„æ—¶é—´ç‚¹ï¼ˆæ”¾åœ¨ä¿ç•™ä¸‹æ¥çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ä¹‹å‰å¾®ç§’çº§ï¼‰
    remaining_messages = [msg for msg in non_summary_messages if msg[0] not in messages_to_compress]
    if remaining_messages:
        earliest_retained_entry = remaining_messages[0][0]
        summary_timestamp = earliest_retained_entry.created_at - timedelta(microseconds=1)
    else:
        # å¦‚æœæ‰€æœ‰æ¶ˆæ¯éƒ½è¢«å‹ç¼©äº†ï¼Œå°±ç”¨å½“å‰æ—¶é—´
        summary_timestamp = datetime.utcnow()

    # æ’å…¥æ‘˜è¦ä½œä¸º System æ¶ˆæ¯
    chat_crud.create_chat_message(
        db,
        session_id,
        "system", 
        f"ã€å†å²æ‘˜è¦ã€‘\n{summary_text}",
        created_at=summary_timestamp,
    )
    db.commit()
    
    logger.info(f"[{session_id}] å‹ç¼©å®Œæˆ: {len(messages_to_compress)} æ¡æ¶ˆæ¯ -> 1 æ¡æ‘˜è¦ï¼ŒèŠ‚çœçº¦ {compressed_tokens} tokens")


async def process_chat(
    db: Session, 
    payload: ChatRequest, 
    background_tasks: BackgroundTasks, 
    model: Optional[str] = None 
) -> ChatResponse:
    # 1. è·å–å¯ç”¨çš„ System Modules (æ—¶é—´ã€å¤©æ°”ç­‰)
    active_modules = (
        db.query(db_models.SystemPromptModule)
        .filter(db_models.SystemPromptModule.is_enabled == True)
        .order_by(db_models.SystemPromptModule.position.asc())
        .all()
    )
    
    # é¢„å¤„ç†ï¼šæ›¿æ¢å˜é‡ (ä¾‹å¦‚ {char_name})
    char_name = payload.card.get("name", "Character") if payload.card else "Character"
    processed_modules = []
    for mod in active_modules:
        try:
            content = mod.content.format(char_name=char_name)
            processed_modules.append(content)
        except KeyError:
            processed_modules.append(mod.content)
            logger.warning(f"æ¨¡å— {mod.name} æ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ã€‚")

    """
    å•è½®æ¶ˆæ¯ä¸»æµç¨‹
    """
    card_id = payload.card.get("id") if payload.card else "default"
    session_id = f"{payload.user_id}::card::{card_id}"
    message = (payload.message or "").strip()
    
    # 2. ç¡®å®šæ¨¡å‹åŠå…¶å‚æ•°é™åˆ¶ (æ ¸å¿ƒä¿®æ”¹ç‚¹)
    model_name = getattr(payload, "model", None) or DEFAULT_MODEL
    
    # å°è¯•ä» Config è·å–è¯¥æ¨¡å‹çš„é™åˆ¶ (å…¼å®¹æ—§ Config)
    if hasattr(settings, "get_model_limit"):
        model_limits = settings.get_model_limit(model_name)
    else:
        # Fallback: å¦‚æœ Config è¿˜æ²¡æ›´æ–°ï¼Œä½¿ç”¨æ—§é€»è¾‘ + é»˜è®¤ Output
        fallback_ctx = getattr(settings, "MAX_MODEL_CONTEXT_LENGTH", 4096)
        model_limits = {
            "context_window": fallback_ctx,
            "max_output": 1024, # é»˜è®¤é¢„ç•™ 1k ç»™å›å¤
            "safety_buffer": 200
        }

    logger.info(f"ğŸ‘‰ [Chatè¯·æ±‚] User: {payload.user_id} | Model: {model_name} | Limits: {model_limits}")

    if not session_id:
        raise HTTPException(status_code=400, detail="session_id ä¸èƒ½ä¸ºç©º")
    if not message:
        raise HTTPException(status_code=400, detail="message ä¸èƒ½ä¸ºç©º")

    # 3. å†å²è®°å½•è·å–ä¸å¤„ç† å·²åºŸå¼ƒ
    history_limit = payload.max_context_messages or DEFAULT_MAX_HISTORY_MSG
    history_limit = max(0, min(history_limit, HARD_MAX_HISTORY_MSG))
    
    fetch_limit = max(history_limit, 1)
    history_rows = chat_crud.get_recent_chat_history(db, session_id, limit=fetch_limit)
    
    # åˆ†ç¦»å†å²æ¶ˆæ¯ä¸­çš„ Summary (System) å’Œå¯¹è¯ (User/Assistant)
    raw_history: List[Dict[str, Any]] = [
        {"id": r.id, "role": r.role, "content": r.content, "created_at": r.created_at}
        for r in history_rows
    ]
    
    clean_history = []
    summary_list = []
    
    for msg in raw_history:
        if msg["role"] == "system" and "æ‘˜è¦" in (msg["content"] or ""):
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºæ‘˜è¦æ¶ˆæ¯ï¼Œæå–å†…å®¹
            summary_list.append(msg["content"])
        else:
            clean_history.append(msg)
            
    history_summary = "\n\n".join(summary_list) if summary_list else None

    # [ä¿®å¤] é‡æ–°å®šä¹‰ history_for_prompt
    history_for_prompt = clean_history[-history_limit:] if history_limit else clean_history

    # 4. RAG è®°å¿†æ£€ç´¢ (Memory)
    relevant_memories = []
    rag_enabled = True
    rag_limit = 5
    
    if payload.memory_config:
        rag_enabled = payload.memory_config.enabled
        rag_limit = payload.memory_config.limit

    if rag_enabled:
        try:
            relevant_memories_objs = await memory_crud.search_memories(
                db, payload.user_id, message, limit=rag_limit
            )
            
            current_tokens = 0
            for m in relevant_memories_objs:
                # ä½¿ç”¨ Token ä¼°ç®—è€Œä¸æ˜¯å­—ç¬¦æ•°
                memory_tokens = _estimate_tokens(m.content)
                if current_tokens + memory_tokens > MAX_MEMORY_TOKENS:
                    break
                relevant_memories.append(m.content)
                current_tokens += memory_tokens
            
            if relevant_memories:
                logger.info(f"[{session_id}] Memory RAG æ³¨å…¥: {len(relevant_memories)} æ¡ (çº¦ {current_tokens} tokens)")
        except Exception as exc:
            logger.warning("Memory RAG æ£€ç´¢å¼‚å¸¸: %s", exc)
            relevant_memories = []

    # 5. Lorebook è‡ªåŠ¨åŠ è½½ä¸æ£€ç´¢
    lore_entries = payload.lore
    if not lore_entries:
        try:
            # Server-side Fetch: ä»…åŠ è½½è¯¥ç”¨æˆ·çš„ Active Entries
            lore_entries = lorebook_crud.get_active_lore_entries(db, payload.user_id)
            if lore_entries:
                logger.info(f"ğŸ“š [Lorebook] å·²åŠ è½½ {len(lore_entries)} æ¡ä¸–ç•Œä¹¦æ¡ç›®")
        except Exception as e:
            logger.warning(f"Lorebook åŠ è½½å¤±è´¥: {e}")
            lore_entries = []

    # æ··åˆæ£€ç´¢: å‘é‡æœç´¢ + å…³é”®è¯åŒ¹é… Lorebook
    # ä¼˜åŒ–ï¼šç›´æ¥è·å–å®Œæ•´æ¡ç›®å¯¹è±¡ï¼Œé¿å…é‡å¤æ£€ç´¢
    vector_entries = []  # å‘é‡å‘½ä¸­çš„å®Œæ•´æ¡ç›®
    keyword_entries = []  # å…³é”®è¯å‘½ä¸­çš„å®Œæ•´æ¡ç›®
    
    # 1. å‘é‡æ£€ç´¢ (è¯­ä¹‰æœç´¢) - ç›´æ¥è·å–å®Œæ•´æ¡ç›®å¯¹è±¡
    if lore_entries and vector_store.is_available():
        active_book_ids = set()
        for e in lore_entries:
            bid = e.get("lorebookId") or e.get("lorebook_id")
            if bid: active_book_ids.add(str(bid))
        
        if active_book_ids:
            try:
                # è¯­ä¹‰æœç´¢ - ä¼ å…¥å®Œæ•´æ¡ç›®åˆ—è¡¨ï¼Œç›´æ¥è¿”å›åŒ¹é…çš„å¯¹è±¡
                vector_matches = await vector_store.search_lore(message, list(active_book_ids), limit=3, all_entries=lore_entries)
                if vector_matches:
                    vector_entries = vector_matches
                    logger.info(f"ğŸ“˜ [Lore RAG] å‘é‡å‘½ä¸­ {len(vector_matches)} æ¡")
            except Exception as e:
                logger.warning(f"Lore RAG Error: {e}")
        else:
            logger.info("â„¹ï¸ [Lore RAG] è·³è¿‡å‘é‡æ£€ç´¢ (æ—  active_book_ids)")

    # 2. å…³é”®è¯æ£€ç´¢ (ç²¾ç¡®åŒ¹é…) - ç›´æ¥è·å–å®Œæ•´æ¡ç›®å¯¹è±¡
    try:
        keyword_matches = lorebook_crud.search_lore_entries_by_keywords(lore_entries, message, limit=5)
        if keyword_matches:
            keyword_entries = keyword_matches
            logger.info(f"ğŸ” [Lore RAG] å…³é”®è¯å‘½ä¸­ {len(keyword_matches)} æ¡")
    except Exception as e:
        logger.warning(f"Lore å…³é”®è¯æ£€ç´¢ Error: {e}")

    # 3. æ„å»º triggered_entries - ç›´æ¥ä»æ£€ç´¢ç»“æœæ„é€ ï¼Œé¿å…é‡å¤éå†
    triggered_entries = []
    seen_ids = set()
    
    # å…ˆæ·»åŠ å‘é‡å‘½ä¸­çš„æ¡ç›®ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
    for entry in vector_entries:
        entry_id = str(entry.get("id"))
        if entry_id not in seen_ids:
            seen_ids.add(entry_id)
            # æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆä½¿ç”¨commentï¼Œå…¶æ¬¡ä½¿ç”¨contentçš„å‰20ä¸ªå­—ç¬¦ï¼‰
            title = entry.get("comment") or ""
            if not title:
                content_preview = entry.get("content", "")[:20]
                title = content_preview + "..." if len(content_preview) >= 20 else content_preview
            
            triggered_entries.append({
                "id": entry_id,
                "content": entry.get("content", ""),
                "type": "vector",
                "title": title,
                "priority": entry.get("priority", 0)
            })
    
    # å†æ·»åŠ å…³é”®è¯å‘½ä¸­çš„æ¡ç›®ï¼ˆé¿å…é‡å¤ï¼‰
    for entry in keyword_entries:
        entry_id = str(entry.get("id"))
        if entry_id not in seen_ids:
            seen_ids.add(entry_id)
            # æå–æ ‡é¢˜
            title = entry.get("comment") or ""
            if not title:
                content_preview = entry.get("content", "")[:20]
                title = content_preview + "..." if len(content_preview) >= 20 else content_preview
            
            triggered_entries.append({
                "id": entry_id,
                "content": entry.get("content", ""),
                "type": "keyword",
                "title": title,
                "priority": entry.get("priority", 0)
            })

    # æ”¶é›†æ‰€æœ‰å‘½ä¸­çš„æ¡ç›®IDç”¨äºrouter_decision
    all_triggered_ids = set()
    for entry in vector_entries + keyword_entries:
        all_triggered_ids.add(str(entry.get("id")))

    if triggered_entries:
        logger.info(f"ğŸ¯ [Lore RAG] æ··åˆæ£€ç´¢æ€»å‘½ä¸­ {len(triggered_entries)} æ¡ (å‘é‡:{len(vector_entries)} + å…³é”®è¯:{len(keyword_entries)})")
    else:
        logger.info(f"ğŸ“­ [Lore RAG] æœªå‘½ä¸­ä»»ä½•æ¡ç›®")

    # 6. æ„å»º Prompt (æ ¸å¿ƒä¿®æ”¹: ä¼ å…¥åŠ¨æ€ Limits)
    # æ³¨æ„: max_context_tokens è¿™é‡Œåªæ˜¯å‰ç«¯ä¼ æ¥çš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬ä¸»è¦ä¾èµ–åç«¯çš„ max_model_tokens æ¥åšç¡¬é™åˆ¶
    user_max_history = payload.max_context_tokens or DEFAULT_MAX_HISTORY_TOKENS

    norm = build_normalized_prompt(
        card=payload.card or {},
        lore_entries=lore_entries or [],
        history=[{"role": i["role"], "content": i["content"]} for i in history_for_prompt],
        user_message=message,
        max_history_tokens=user_max_history,           # è½¯ä¸Šé™: å†å²è®°å½•æœŸæœ›é•¿åº¦
        memories=relevant_memories,
        history_summary=history_summary, 
        system_modules=processed_modules,
        router_decision={"rag_lore_ids": list(all_triggered_ids)}
    )

    openai_payload = to_openai_payload(norm, model_name)
    
    # [DEBUG] æ‰“å° Token ç»Ÿè®¡ (å¦‚æœ build_normalized_prompt è¿”å›äº†çš„è¯)
    if "tokenStats" in norm:
        stats = norm["tokenStats"]
        logger.info(f"ğŸ“Š Tokené¢„ç®—: Sys={stats['system']} | User={stats['user']} | Hist={stats['history']} | Left={stats['budget_left']}")

    try:
        reply_content = await call_llm(
            model=model_name, 
            messages=openai_payload["messages"],
            temperature=payload.temperature,
            top_p=payload.top_p,
            max_tokens=payload.max_tokens, # è¿™é‡Œæ˜¯è®© LLM çŸ¥é“ä»€ä¹ˆæ—¶å€™åœæ­¢ï¼Œé€šå¸¸ <= max_output
            frequency_penalty=payload.frequency_penalty,
            presence_penalty=payload.presence_penalty,
        )
        
        print("\n" + "="*40)
        print(f"ğŸ§  [LLM åŸå§‹å›å¤]\n{reply_content}")
        print("="*40 + "\n")
        logger.info("âœ… LLM å“åº”æˆåŠŸ")

    except Exception as exc:
        logger.exception("LLM è°ƒç”¨å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"LLM Error: {exc}")

    try:
        # å†™å…¥æ•°æ®åº“
        chat_crud.create_chat_message(db, session_id, "user", message)
        chat_crud.create_chat_message(db, session_id, "assistant", reply_content)
        db.commit() 

        background_tasks.add_task(_run_compact_history_task, session_id)
        
    except Exception as exc:
        logger.error("DB å†™å…¥å¤±è´¥: %s", exc)

    return ChatResponse(
        reply=reply_content,
        systemPreview=norm.get("systemPrompt"),
        usedLore=norm.get("loreBlock"),
        triggered_entries=triggered_entries if triggered_entries else None,
        triggeredLoreItems=norm.get("triggeredLore"),
        tokenStats=norm.get("tokenStats"),
    )
