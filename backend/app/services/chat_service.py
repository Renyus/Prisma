# backend/app/services/chat_service.py

from __future__ import annotations

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from fastapi import HTTPException, BackgroundTasks
from sqlalchemy.orm import Session

from app.core.config import settings
from app.core.llm import call_llm, call_summary_llm
from app.core.vector_store import vector_store
from app.db import models as db_models
from app.schemas.chat import ChatRequest, ChatResponse
from app.services.payload_builder import to_openai_payload
from app.services.prompt_builder import build_normalized_prompt
from app.crud import chat as chat_crud
from app.crud import memory as memory_crud
from app.crud import lorebook as lorebook_crud

# å¼ºåˆ¶æ˜¾ç¤º INFO æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é»˜è®¤é…ç½®
DEFAULT_MAX_HISTORY_MSG = 30
HARD_MAX_HISTORY_MSG = 100
DEFAULT_MAX_HISTORY_TOKENS = 2400
DEFAULT_MODEL = settings.CHAT_MODEL
MAX_MEMORY_CHARS = 2000 

async def _run_compact_history_task(db: Session, session_id: str):
    """
    åå°å‹ç¼©å†å²è®°å½•ä»»åŠ¡
    æ³¨æ„: è¿™é‡Œçš„ db session å¿…é¡»ç¡®ä¿åœ¨ä»»åŠ¡æ‰§è¡Œæ—¶æœªè¢«å…³é—­ã€‚
    """
    try:
        await _maybe_compact_history(db, session_id)
    except Exception as e:
        logger.error(f"åå°æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")

async def _maybe_compact_history(db: Session, session_id: str) -> None:
    threshold = settings.SUMMARY_HISTORY_THRESHOLD
    if threshold <= 0:
        return

    total_count = (
        db.query(db_models.ChatMessage)
        .filter(db_models.ChatMessage.user_id == session_id)
        .count()
    )
    excess = max(total_count - threshold, 0)
    if excess <= 0:
        return

    # è·å–æœ€æ—§çš„æ¶ˆæ¯ç”¨äºç”Ÿæˆæ‘˜è¦
    oldest_entries = (
        db.query(db_models.ChatMessage)
        .filter(db_models.ChatMessage.user_id == session_id)
        .order_by(db_models.ChatMessage.created_at.asc())
        .limit(excess)
        .all()
    )
    if not oldest_entries:
        return

    summary_sources = [
        {"role": entry.role, "content": entry.content}
        for entry in oldest_entries
        if entry.content
    ]
    if not summary_sources:
        return

    try:
        summary_text = (await call_summary_llm(summary_sources)).strip()
    except Exception as exc:
        logger.warning("Summary å‹ç¼©å¤±è´¥: %s", exc)
        return

    if not summary_text:
        return

    # åˆ é™¤æ—§æ¶ˆæ¯
    ids_to_delete = [entry.id for entry in oldest_entries if entry.id]
    if not ids_to_delete:
        return

    chat_crud.delete_chat_messages_by_ids(db, ids_to_delete)

    # è®¡ç®—æ‘˜è¦æ’å…¥çš„æ—¶é—´ç‚¹ï¼ˆæ”¾åœ¨ä¿ç•™ä¸‹æ¥çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ä¹‹å‰å¾®ç§’çº§ï¼‰
    earliest_retained_entry = (
        db.query(db_models.ChatMessage)
        .filter(db_models.ChatMessage.user_id == session_id)
        .order_by(db_models.ChatMessage.created_at.asc())
        .offset(excess)
        .limit(1)
        .first()
    )

    summary_timestamp = datetime.utcnow()
    if earliest_retained_entry and earliest_retained_entry.created_at:
         summary_timestamp = earliest_retained_entry.created_at - timedelta(microseconds=1)

    # æ’å…¥æ‘˜è¦ä½œä¸º System æ¶ˆæ¯ (æˆ–è€…ä¸“é—¨çš„ summary ç±»å‹ï¼Œå–å†³äºä½ çš„å®ç°ï¼Œè¿™é‡Œä¿æŒ System)
    chat_crud.create_chat_message(
        db,
        session_id,
        "system", 
        f"ã€å†å²æ‘˜è¦ã€‘\n{summary_text}",
        created_at=summary_timestamp,
    )
    db.commit()


async def process_chat(
    db: Session, 
    payload: ChatRequest, 
    background_tasks: BackgroundTasks, 
    model: Optional[str] = None 
) -> ChatResponse:
    # 1. è·å–å¯ç”¨çš„ System Modules (æ—¶é—´ã€å¤©æ°”ç­‰)
    active_modules = (
        db.query(db_models.SystemPromptModule)
        .filter(db_models.SystemPromptModule.is_enabled == True)
        .order_by(db_models.SystemPromptModule.position.asc())
        .all()
    )
    
    # é¢„å¤„ç†ï¼šæ›¿æ¢å˜é‡ (ä¾‹å¦‚ {char_name})
    char_name = payload.card.get("name", "Character") if payload.card else "Character"
    processed_modules = []
    for mod in active_modules:
        try:
            content = mod.content.format(char_name=char_name)
            processed_modules.append(content)
        except KeyError:
            processed_modules.append(mod.content)
            logger.warning(f"æ¨¡å— {mod.name} æ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ã€‚")

    """
    å•è½®æ¶ˆæ¯ä¸»æµç¨‹
    """
    card_id = payload.card.get("id") if payload.card else "default"
    session_id = f"{payload.user_id}::card::{card_id}"
    message = (payload.message or "").strip()
    
    # 2. ç¡®å®šæ¨¡å‹åŠå…¶å‚æ•°é™åˆ¶ (æ ¸å¿ƒä¿®æ”¹ç‚¹)
    model_name = getattr(payload, "model", None) or DEFAULT_MODEL
    
    # å°è¯•ä» Config è·å–è¯¥æ¨¡å‹çš„é™åˆ¶ (å…¼å®¹æ—§ Config)
    if hasattr(settings, "get_model_limit"):
        model_limits = settings.get_model_limit(model_name)
    else:
        # Fallback: å¦‚æœ Config è¿˜æ²¡æ›´æ–°ï¼Œä½¿ç”¨æ—§é€»è¾‘ + é»˜è®¤ Output
        fallback_ctx = getattr(settings, "MAX_MODEL_CONTEXT_LENGTH", 4096)
        model_limits = {
            "context_window": fallback_ctx,
            "max_output": 1024, # é»˜è®¤é¢„ç•™ 1k ç»™å›å¤
            "safety_buffer": 200
        }

    logger.info(f"ğŸ‘‰ [Chatè¯·æ±‚] User: {payload.user_id} | Model: {model_name} | Limits: {model_limits}")

    if not session_id:
        raise HTTPException(status_code=400, detail="session_id ä¸èƒ½ä¸ºç©º")
    if not message:
        raise HTTPException(status_code=400, detail="message ä¸èƒ½ä¸ºç©º")

    # 3. å†å²è®°å½•è·å–ä¸å¤„ç†
    history_limit = payload.max_context_messages or DEFAULT_MAX_HISTORY_MSG
    history_limit = max(0, min(history_limit, HARD_MAX_HISTORY_MSG))
    
    fetch_limit = max(history_limit, 1)
    history_rows = chat_crud.get_recent_chat_history(db, session_id, limit=fetch_limit)
    
    # åˆ†ç¦»å†å²æ¶ˆæ¯ä¸­çš„ Summary (System) å’Œå¯¹è¯ (User/Assistant)
    raw_history: List[Dict[str, Any]] = [
        {"id": r.id, "role": r.role, "content": r.content, "created_at": r.created_at}
        for r in history_rows
    ]
    
    clean_history = []
    summary_list = []
    
    for msg in raw_history:
        if msg["role"] == "system" and "æ‘˜è¦" in (msg["content"] or ""):
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºæ‘˜è¦æ¶ˆæ¯ï¼Œæå–å†…å®¹
            summary_list.append(msg["content"])
        else:
            clean_history.append(msg)
            
    history_summary = "\n\n".join(summary_list) if summary_list else None

    # [ä¿®å¤] é‡æ–°å®šä¹‰ history_for_prompt
    history_for_prompt = clean_history[-history_limit:] if history_limit else clean_history

    # 4. RAG è®°å¿†æ£€ç´¢ (Memory)
    relevant_memories = []
    rag_enabled = True
    rag_limit = 5
    
    if payload.memory_config:
        rag_enabled = payload.memory_config.enabled
        rag_limit = payload.memory_config.limit

    if rag_enabled:
        try:
            relevant_memories_objs = await memory_crud.search_memories(
                db, payload.user_id, message, limit=rag_limit
            )
            
            current_chars = 0
            for m in relevant_memories_objs:
                if current_chars + len(m.content) > MAX_MEMORY_CHARS:
                    break
                relevant_memories.append(m.content)
                current_chars += len(m.content)
            
            if relevant_memories:
                logger.info(f"[{session_id}] Memory RAG æ³¨å…¥: {len(relevant_memories)} æ¡")
        except Exception as exc:
            logger.warning("Memory RAG æ£€ç´¢å¼‚å¸¸: %s", exc)
            relevant_memories = []

    # 5. Lorebook è‡ªåŠ¨åŠ è½½ä¸æ£€ç´¢
    lore_entries = payload.lore
    if not lore_entries:
        try:
            # Server-side Fetch: ä»…åŠ è½½è¯¥ç”¨æˆ·çš„ Active Entries
            lore_entries = lorebook_crud.get_active_lore_entries(db, payload.user_id)
            if lore_entries:
                logger.info(f"ğŸ“š [Lorebook] å·²åŠ è½½ {len(lore_entries)} æ¡ä¸–ç•Œä¹¦æ¡ç›®")
        except Exception as e:
            logger.warning(f"Lorebook åŠ è½½å¤±è´¥: {e}")
            lore_entries = []

    # æ··åˆæ£€ç´¢: å‘é‡æœç´¢ Lorebook
    rag_lore_ids = []
    if lore_entries and vector_store.is_available():
        active_book_ids = set()
        for e in lore_entries:
            bid = e.get("lorebookId") or e.get("lorebook_id")
            if bid: active_book_ids.add(str(bid))
        
        if active_book_ids:
            try:
                # è¯­ä¹‰æœç´¢
                rag_lore_ids = await vector_store.search_lore(message, list(active_book_ids), limit=3)
                if rag_lore_ids:
                    logger.info(f"ğŸ“˜ [Lore RAG] å‘é‡å‘½ä¸­ {len(rag_lore_ids)} æ¡")
            except Exception as e:
                logger.warning(f"Lore RAG Error: {e}")

    # 6. æ„å»º Prompt (æ ¸å¿ƒä¿®æ”¹: ä¼ å…¥åŠ¨æ€ Limits)
    # æ³¨æ„: max_context_tokens è¿™é‡Œåªæ˜¯å‰ç«¯ä¼ æ¥çš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬ä¸»è¦ä¾èµ–åç«¯çš„ max_model_tokens æ¥åšç¡¬é™åˆ¶
    user_max_history = payload.max_context_tokens or DEFAULT_MAX_HISTORY_TOKENS

    norm = build_normalized_prompt(
        card=payload.card or {},
        lore_entries=lore_entries or [],
        history=[{"role": i["role"], "content": i["content"]} for i in history_for_prompt],
        user_message=message,
        
        # --- æ ¸å¿ƒå˜æ›´å¼€å§‹ ---
        max_history_tokens=user_max_history,           # è½¯ä¸Šé™: å†å²è®°å½•æœŸæœ›é•¿åº¦
        max_model_tokens=model_limits["context_window"], # ç¡¬ä¸Šé™: æ¨¡å‹æ€»çª—å£
        max_output_tokens=model_limits["max_output"],    # ç¡¬ä¸Šé™: é¢„ç•™ç»™å›å¤çš„ç©ºé—´
        # --- æ ¸å¿ƒå˜æ›´ç»“æŸ ---
        
        memories=relevant_memories,
        history_summary=history_summary, 
        system_modules=processed_modules,
        router_decision={"rag_lore_ids": rag_lore_ids}
    )

    openai_payload = to_openai_payload(norm, model_name)
    
    # [DEBUG] æ‰“å° Token ç»Ÿè®¡ (å¦‚æœ build_normalized_prompt è¿”å›äº†çš„è¯)
    if "tokenStats" in norm:
        stats = norm["tokenStats"]
        logger.info(f"ğŸ“Š Tokené¢„ç®—: Sys={stats['system']} | User={stats['user']} | Hist={stats['history']} | Left={stats['budget_left']}")

    try:
        reply_content = await call_llm(
            model=model_name, 
            messages=openai_payload["messages"],
            temperature=payload.temperature,
            top_p=payload.top_p,
            max_tokens=payload.max_tokens, # è¿™é‡Œæ˜¯è®© LLM çŸ¥é“ä»€ä¹ˆæ—¶å€™åœæ­¢ï¼Œé€šå¸¸ <= max_output
            frequency_penalty=payload.frequency_penalty,
            presence_penalty=payload.presence_penalty,
        )
        
        print("\n" + "="*40)
        print(f"ğŸ§  [LLM åŸå§‹å›å¤]\n{reply_content}")
        print("="*40 + "\n")
        logger.info("âœ… LLM å“åº”æˆåŠŸ")

    except Exception as exc:
        logger.exception("LLM è°ƒç”¨å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"LLM Error: {exc}")

    try:
        # å†™å…¥æ•°æ®åº“
        chat_crud.create_chat_message(db, session_id, "user", message)
        chat_crud.create_chat_message(db, session_id, "assistant", reply_content)
        db.commit() 

        background_tasks.add_task(_run_compact_history_task, db, session_id)
        
    except Exception as exc:
        logger.error("DB å†™å…¥å¤±è´¥: %s", exc)

    return ChatResponse(
        reply=reply_content,
        systemPreview=norm.get("systemPrompt"),
        usedLore=norm.get("loreBlock"),
    )