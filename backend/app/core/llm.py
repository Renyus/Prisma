import httpx
from typing import List, Dict, Optional, Any
import json
from app.core.config import settings

# å°† .env é…ç½®æ˜ å°„åˆ°å†…éƒ¨é€»è¾‘åç§°
GENERATION_MODEL = settings.CHAT_MODEL
SUMMARY_MODEL = settings.UTILITY_MODEL
MEMORY_MODEL = settings.UTILITY_MODEL # è®°å¿†å’Œæ€»ç»“å…±ç”¨ UTILITY_MODEL

def get_model_config(model_name: str) -> Dict[str, Any]:
    """
    æ ¹æ®æ¨¡åž‹åç§°åŠ¨æ€åŒ¹é…é…ç½®ã€‚
    """
    # 1. å¦‚æžœæ˜¯ Chat æ¨¡åž‹
    if model_name == settings.CHAT_MODEL:
        key, url = settings.CHAT_CREDENTIALS
        return _fmt_config(key, url, temp=0.7)
    
    # 2. å¦‚æžœæ˜¯ Utility æ¨¡åž‹
    if model_name == settings.UTILITY_MODEL:
        key, url = settings.UTILITY_CREDENTIALS
        return _fmt_config(key, url, temp=0.1)
    
    # 3. æœªçŸ¥æ¨¡åž‹ (å¯èƒ½æ˜¯å‰ç«¯ä¼ æ¥çš„è‡ªå®šä¹‰æ¨¡åž‹ï¼Œæˆ–è€…æ—§çš„é»˜è®¤å€¼)
    # é»˜è®¤ä½¿ç”¨ Chat çš„é…ç½®å°è¯•åŽ»è·‘
    key, url = settings.CHAT_CREDENTIALS
    return _fmt_config(key, url, temp=0.7)

def _fmt_config(key: str, base_url: str, temp: float) -> Dict[str, Any]:
    # è‡ªåŠ¨ä¿®æ­£ URLï¼šå¦‚æžœä»¥ /v1 ç»“å°¾ï¼Œè¡¥ä¸Š /chat/completions
    # å¦‚æžœå·²ç»æ˜¯ /chat/completions ç»“å°¾ï¼Œä¿æŒä¸å˜
    # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å®¹é”™å¤„ç†
    url = base_url
    if not url.endswith("/chat/completions"):
        url = url.rstrip("/") + "/chat/completions"
    
    return {
        "api_key": key,
        "api_url": url,
        "default_temp": temp,
        "base_url": base_url # åŽŸå§‹ Base URLï¼Œä¾› list_models ä½¿ç”¨
    }

def standardize_usage(raw_usage: Dict[str, Any], model: str) -> Dict[str, int]:
    """
    æ ‡å‡†åŒ–ä¸åŒä¾›åº”å•†çš„ usage å­—æ®µåä¸ºç»Ÿä¸€æ ¼å¼ã€‚
    
    Args:
        raw_usage: åŽŸå§‹ usage æ•°æ®
        model: æ¨¡åž‹åç§°ï¼Œç”¨äºŽè¯†åˆ«ä¾›åº”å•†
    
    Returns:
        Dict[str, int]: æ ‡å‡†åŒ–æ ¼å¼ {"cacheHit": int, "cacheMiss": int, "total": int}
    """
    


    # é»˜è®¤å€¼
    standardized = {
        "cacheHit": 0,
        "cacheMiss": 0,
        "total": 0
    }
    
    if not raw_usage:
        return standardized
    
    # æ ¹æ®æ¨¡åž‹åç§°åˆ¤æ–­ä¾›åº”å•†
    model_lower = model.lower()
    
    # DeepSeek ç›¸å…³å­—æ®µæ˜ å°„
    if "deepseek" in model_lower:
        # DeepSeek ä½¿ç”¨ prompt_cache_hit_tokens è¡¨ç¤ºç¼“å­˜å‘½ä¸­
        cache_hit = raw_usage.get("prompt_cache_hit_tokens", 0)
        # è®¡ç®—ç¼“å­˜æœªå‘½ä¸­ = æ€»è¾“å…¥tokens - ç¼“å­˜å‘½ä¸­tokens
        total_input = raw_usage.get("prompt_tokens", 0)
        cache_miss = max(0, total_input - cache_hit)
        total = raw_usage.get("total_tokens", 0)
        
        standardized.update({
            "cacheHit": cache_hit,
            "cacheMiss": cache_miss,
            "total": total
        })
    
    # Claude ç›¸å…³å­—æ®µæ˜ å°„
    elif "claude" in model_lower:
        # Claude ä½¿ç”¨ cache_read_input_tokens è¡¨ç¤ºç¼“å­˜è¯»å–
        cache_hit = raw_usage.get("cache_read_input_tokens", 0)
        # è®¡ç®—ç¼“å­˜æœªå‘½ä¸­ = æ€»è¾“å…¥tokens - ç¼“å­˜å‘½ä¸­tokens
        total_input = raw_usage.get("input_tokens", 0)
        cache_miss = max(0, total_input - cache_hit)
        total = raw_usage.get("total_tokens", 0)
        
        standardized.update({
            "cacheHit": cache_hit,
            "cacheMiss": cache_miss,
            "total": total
        })
    
    # Gemini ç›¸å…³å­—æ®µæ˜ å°„
    elif "gemini" in model_lower:
        # Gemini çš„å­—æ®µåå¯èƒ½éœ€è¦æ ¹æ®å®žé™…APIå“åº”è°ƒæ•´
        cache_hit = raw_usage.get("cached_content_tokens", 0)
        total_input = raw_usage.get("prompt_tokens", 0)
        cache_miss = max(0, total_input - cache_hit)
        total = raw_usage.get("total_tokens", 0)
        
        standardized.update({
            "cacheHit": cache_hit,
            "cacheMiss": cache_miss,
            "total": total
        })
    
    # å…¶ä»–ä¾›åº”å•†çš„é€šç”¨å¤„ç†
    else:
        # å°è¯•ä»Žå¸¸è§çš„å­—æ®µåä¸­æå–æ•°æ®
        cache_hit = (
            raw_usage.get("prompt_cache_hit_tokens", 0) or
            raw_usage.get("cache_read_input_tokens", 0) or
            raw_usage.get("cached_content_tokens", 0) or
            0
        )
        
        total_input = (
            raw_usage.get("prompt_tokens", 0) or
            raw_usage.get("input_tokens", 0) or
            0
        )
        
        cache_miss = max(0, total_input - cache_hit)
        total = raw_usage.get("total_tokens", 0)
        
        standardized.update({
            "cacheHit": cache_hit,
            "cacheMiss": cache_miss,
            "total": total
        })
    
    return standardized

SUMMARY_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ RPG å™äº‹æ¡£æ¡ˆå‘˜ã€‚
ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»ã€è¿‡å¾€å‰§æƒ…å›žé¡¾ã€‘å’Œã€æ–°å¢žå¯¹è¯ç‰‡æ®µã€‘ï¼Œç”Ÿæˆä¸€ä»½æ›´æ–°åŽçš„ã€å½“å‰å™äº‹æ¡£æ¡ˆã€‘ã€‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿ä¿¡æ¯çš„è¿žç»­æ€§ï¼š

ã€æ ¸å¿ƒå‰§æƒ…æ‘˜è¦ã€‘
(ç”¨ä¸€å¥è¯æ¦‚æ‹¬å½“å‰æ‰€å¤„çš„é˜¶æ®µå’Œå‘ç”Ÿçš„é‡å¤§äº‹ä»¶)

ã€äººç‰©å…³ç³»ä¸ŽçŠ¶æ€ã€‘
- ä¸»ä½“çŠ¶æ€: (å¦‚ï¼šRenyus ç›®å‰å·¦è‡‚å—è½»ä¼¤ï¼ŒçµåŠ›ç›ˆæ»¡)
- å…³ç³»åŠ¨æ€: (å¦‚ï¼š{char_name} å¯¹ç”¨æˆ·äº§ç”Ÿäº†ä¸€ä¸å¥½å¥‡ï¼Œæ€åº¦ä»Žå†·æ·¡è½¬ä¸ºå®¡è§†)

ã€å…³é”®çº¿ç´¢ä¸Žé“å…·ã€‘
- å…³é”®ç‰©å“: (å¦‚ï¼šèŽ·å–äº†é’é“œé’¥åŒ™ã€ä¸¢å¤±äº†åœ°å›¾)
- æœªå®Œå¾…ç»­: (å¦‚ï¼šæ­£å‡†å¤‡å‰å¾€åŽå±±ç¦åœ°ï¼Œå°šæœªå‘ŠçŸ¥å¸ˆçˆ¶çœŸç›¸)

ã€çŽ¯å¢ƒä¸Šä¸‹æ–‡ã€‘
- æ—¶é—´/åœ°ç‚¹: (å¦‚ï¼šé»„æ˜ã€ç ´æ—§çš„é“è§‚å†…)

æ³¨æ„ï¼š
1. ä¿æŒç®€æ´ï¼Œå‰”é™¤æ— æ„ä¹‰çš„å¯’æš„ã€‚
2. å¦‚æžœã€è¿‡å¾€å‰§æƒ…å›žé¡¾ã€‘ä¸­æåˆ°çš„é‡è¦ä¿¡æ¯å°šæœªè§£å†³ï¼Œå¿…é¡»ä¿ç•™åˆ°æ–°æ¡£æ¡ˆä¸­ã€‚
3. ç›´æŽ¥è¿”å›žæ ¼å¼åŒ–åŽçš„æ–‡æœ¬ã€‚"""

async def call_llm(
    model: str, 
    messages: List[Dict[str, str]], 
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    max_tokens: Optional[int] = None,
    frequency_penalty: Optional[float] = None,
    presence_penalty: Optional[float] = None,
) -> Dict[str, Any]:
    """
    ç»Ÿä¸€ LLM è°ƒç”¨å…¥å£ï¼Œè¿”å›žåŒ…å« content å’Œ usage çš„ç»“æž„åŒ–æ•°æ®
    """
    result = await call_chat_completions(
        model, messages, temperature, top_p, max_tokens, frequency_penalty, presence_penalty
    )
    return result

async def call_summary_llm(history: List[Dict[str, str]], char_name: str = "è§’è‰²") -> str:
    """è°ƒç”¨ Utility æ¨¡åž‹è¿›è¡Œæ€»ç»“"""
    history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
    messages = [
        # âœ… ä½¿ç”¨ .format å¡«å……å ä½ç¬¦
        {"role": "system", "content": SUMMARY_SYSTEM_PROMPT.format(char_name=char_name)}, 
        {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯åŽ†å²ï¼š\n---\n{history_text}"},
    ]
    try:
        result = await call_chat_completions(
            model=settings.UTILITY_MODEL, 
            messages=messages,
            temperature=0.1,
            max_tokens=1024
        )
        # å¯¹äºŽæ€»ç»“åŠŸèƒ½ï¼Œåªè¿”å›ž content éƒ¨åˆ†
        return result["content"]
    except Exception as e:
        print(f"Summary failed: {e}")
        return ""

async def call_chat_completions(
    model: str, 
    messages: List[Dict[str, str]], 
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    max_tokens: Optional[int] = None,
    frequency_penalty: Optional[float] = None,
    presence_penalty: Optional[float] = None,
) -> Dict[str, Any]:
    config = get_model_config(model)
    api_key = config["api_key"]
    url = config["api_url"]
    
    if not api_key:
        raise RuntimeError(f"Model {model} API Key not configured.")

    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature if temperature is not None else config["default_temp"],
        "max_tokens": max_tokens if max_tokens is not None else 2048,
    }
    if top_p is not None: payload["top_p"] = top_p
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    async with httpx.AsyncClient(timeout=300) as client:
        resp = await client.post(url, json=payload, headers=headers)
    
    try:
        data = resp.json()
    except:
        data = resp.text

    if resp.status_code != 200:
        err = data.get("error") if isinstance(data, dict) else data
        raise RuntimeError(f"LLM API Error ({model}): {err}")

    # æå– content
    content = data["choices"][0]["message"]["content"]
    
    # æå–å¹¶æ ‡å‡†åŒ– usage æ•°æ®
    raw_usage = data.get("usage", {})
    # ðŸ‘‡ðŸ‘‡ðŸ‘‡ ã€æ–°å¢žè°ƒè¯•æ‰“å°ã€‘ ðŸ‘‡ðŸ‘‡ðŸ‘‡
    print("\n" + ">" * 20 + " [USAGE DEBUG] " + "<" * 20)
    print(f"ðŸ“¦ æ¨¡åž‹: {model}")
    print(f"ðŸ§¾ åŽŸå§‹ Usage: {json.dumps(raw_usage, indent=2)}") 
    # ðŸ‘†ðŸ‘†ðŸ‘† è¿™æ ·æˆ‘ä»¬å°±èƒ½çœ‹åˆ° API åˆ°åº•ç»™äº†ä»€ä¹ˆå­—æ®µ
    standardized_usage = standardize_usage(raw_usage, model)
    
    # è¿”å›žç»“æž„åŒ–æ•°æ®
    return {
        "content": content,
        "usage": standardized_usage
    }
